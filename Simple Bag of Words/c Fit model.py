# Code to fit a model
#
# Created by Greg on 27/07/21
#
# Takes as input one or more files in the format generated by Data Preprocessing
# And, the tokenized dictionary generated in 'Tokenization.py'

import json
from pathlib import Path
import time
from keras.preprocessing.text import Tokenizer, tokenizer_from_json
from keras.models import Sequential
from keras.layers import Dense

# Define data source and target
pre_processed_data_source_folder = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/pre_processed")
# files_to_process = ["pre_processed_group_0.txt", "pre_processed_group_1.txt", "pre_processed_group_2.txt", "pre_processed_group_3.txt", "pre_processed_group_4.txt", "pre_processed_group_5.txt",  "pre_processed_group_6.txt", "pre_processed_group_7.txt"]
# Use the below instead of the above for testing			 
files_to_process = ["pre_processed_group_0.txt"]

# Load dictonary source
tokenized_dictionary = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/simple_BOW/tokenized_dictionary.json")
with open(tokenized_dictionary) as f:
    tokenizer = tokenizer_from_json(f.read())
print(f"Loaded tokenized dictionary based on {tokenizer.document_count} reviews, and icluding {len(tokenizer.word_index)} words")

