# Code to generate a tokenized dictionary from the source.
#
# Created by Greg on 27/07/21
#
# Takes as input one or more files in the format generated by Data Preprocessing
# Tokenises using Keras 'Tokenizer' (see https://keras.io/api/preprocessing/text/#tokenizer)
# Spits out some statistics
# Stores the result as a JSON (see https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)
#
# NOTES
# Need to consider performance on words not in corpus vs training set.  Good use for the first test dat vs. second.
# So for now, build dictionary by tokenizing files 0-7, leaving out 8 and 9
# 
# Use of Tokenizer is fine for a test, but appers to require the entire corpus in one go, though also posisble it could use an interable, so present it as a stream
# BUt, try out to see if this is a material issue

import json
from pathlib import Path
import time
from keras.preprocessing.text import Tokenizer

# Define data source and target
pre_processed_data_source_folder = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/pre_processed")
# files_to_load = ["pre_processed_group_0.txt", "pre_processed_group_1.txt", "pre_processed_group_2.txt", "pre_processed_group_3.txt", "pre_processed_group_4.txt", "pre_processed_group_5.txt",  "pre_processed_group_6.txt", "pre_processed_group_7.txt"]
# Use the below instead of the above for testing			 
files_to_load = ["pre_processed_group_0.txt"]

data_target_file = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/simple_BOW/tokenized_dictionary.json")

# List to hold the corpus
corpus = []

# Read in the text corpus
# This is now a list of JSONs, so needs to be read in accordingly
startTime = time.time()
process_count = 0

for current_file in files_to_load:
    print(f"Starting load of {current_file}...")
    file_to_read = pre_processed_data_source_folder / current_file
    with open(file_to_read, mode='r') as file:
        
        process_count_file = 0
        
        for line in file:
            process_count_file +=1
            review = json.loads(line.strip())
            # Concatenate the three string fields into one field, then add that.  Done that way so that TFIDF aligns to the right number of documents
            # Uses 'movie_hidden_years' which hides the year of the film.  'movie' remains available for checking
            review_strings = review['movie_hidden_years'] + review['review_summary'] + review['review_detail']
            corpus.append(review_strings)
        
        print(f"Loaded {process_count_file} lines")
        process_count += process_count_file

print(f"File loads complete after {time.time() - startTime:.2f} seconds. {process_count} reviews loaded")

# create the tokenizer
# Most parameters are default, aside from the oov-token which is set to a specific entry so that rare words are visible
# List usage constrained to 10k words for now.  set below to 0 to use entire word list
max_words = 10000
t = Tokenizer(num_words=max_words, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower=True, split=' ', char_level=False, oov_token='oov_token')

# fit the tokenizer on the documents
print(f"Fitting tokenizer")
t.fit_on_texts(corpus)
print(f"Tokenizer fitted after {time.time() - startTime:.2f} seconds.")

# Show some metrics
print(f"Processed {t.document_count} reviews")
print(f"Found {len(t.word_index)} discrete words")

# Dump the result as a JSON
tokenizer_json = t.to_json(indent=4)
with open(data_target_file, 'w', encoding='utf-8') as f:
    f.write(tokenizer_json + '\n')