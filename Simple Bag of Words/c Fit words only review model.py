# Code to fit a model
#
# Created by Greg on 12/08/21
#
# Takes as input one or more files in the format generated by Data Preprocessing
# And, the tokenized dictionary generated in 'Tokenization.py'
# Model in this case uses a tokenized version of the review, not just word frequency

import json
import time
from pathlib import Path

import matplotlib.pyplot as plt
from numpy import array, argmax
import numpy
from sklearn.metrics import confusion_matrix
from sklearn.utils.validation import check_memory
from tensorflow.keras.layers import BatchNormalization, Dense, InputLayer
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.python.keras.constraints import MaxNorm
from tensorflow.python.keras.layers.core import Dropout

startTime = time.time()

# Load dictonary source
tokenized_dictionary = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/simple_BOW/tokenized_dictionary.json")
with open(tokenized_dictionary) as f:
    tokenizer = tokenizer_from_json(f.read())
print(f"Loaded tokenized dictionary based on {tokenizer.document_count} reviews, and including {len(tokenizer.word_index)} words")

# Define training data source
data_source_folder = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/simple_BOW")
training_files_to_load = ["BOW_training_data.txt"]

# Define the target
model_save_file = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/words_only/saved_model.tf")

# Load the training data
training_texts = []
training_outcomes = []
process_count = 0
# non_number_count = 0
for current_file in training_files_to_load:
    print(f"Starting load of training data by {current_file}...")
    file_to_read = data_source_folder / current_file
    with open(file_to_read, mode='r') as file:        
        for line in file:
            review = json.loads(line.strip())
            # Use both the review summary and the review detail
            # Temporarily switch to using the rating (in bins of positive,neutral, negative) rather than year, since year not proving easy
            # Training outcomes are zero based by subtracting 1998
            # training_outcomes.append (int(review['review_year'])-1998)
            # if (review['rating'] == None):
            #     non_number_count += 1
            #     continue
            # elif (int(review['rating'])>6):
            #     training_outcomes.append (2)
            # elif (int(review['rating'])<4):
            #     training_outcomes.append (0)
            # else:
            #     training_outcomes.append (1)

            # Load the training texts
            training_texts.append (review['review_detail'])

            # Load the outcomes
            training_outcomes.append (review['bin_id'])

            process_count += 1

print(f"{process_count} items of training data loaded after {time.time() - startTime:.2f} seconds")
# print(f"Found {non_number_count} ratings that have a non string rating")

# Define test data source
data_source_folder = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/simple_BOW")
test_files_to_load = ["BOW_test_data.txt"]

# Load the test data
test_texts = []
test_outcomes = []
process_count = 0
# non_number_count = 0
for current_file in test_files_to_load:
    print(f"Starting load of training data by {current_file}...")
    file_to_read = data_source_folder / current_file
    with open(file_to_read, mode='r') as file:        
        for line in file:
            review = json.loads(line.strip())
           
            # Load the test texts
            test_texts.append (review['review_detail'])

            # Load the outcomes
            test_outcomes.append (review['bin_id'])

            process_count += 1

print(f"{process_count} items of test data loaded after {time.time() - startTime:.2f} seconds")

# Set the desired dictionary size
max_words = 1500
tokenizer.num_words = max_words

# Build the tokenized training texts
max_review_words = 1500
print(f"Tokenizing training texts, with length of {max_review_words} and dictionary of {max_words}")
full_reviews = tokenizer.texts_to_sequences(training_texts)
Xtrain = pad_sequences(full_reviews, max_review_words, padding='post')
print(f"Tokenizing training texts complete after {time.time() - startTime:.2f} seconds")
print(f"Training texts shape is {Xtrain.shape}")
print(f"Training texts example is {Xtrain[0]}")

# Buld the target training matrix
# Target is an ordinal variable -
# But, for initial version treat as a classification
# Uses the Keras to_categorical model
Ytrain = array(training_outcomes)
print(f"Training outcomes shape is {Ytrain.shape}")

# Build the tokenized test texts
print(f"Tokenizing training texts, with length of {max_review_words} and dictionary of {max_words}")
full_reviews = tokenizer.texts_to_sequences(test_texts)
Xtest = pad_sequences(full_reviews, max_review_words, padding='post')
print(f"Tokenizing test texts complete after {time.time() - startTime:.2f} seconds")
print(f"Test texts shape is {Xtest.shape}")
print(f"Test texts example is {Xtest[0]}")

# Buld the target test matrix
# Target is an ordinal variable -
# But, for initial version treat as a classification
# Uses the Keras to_categorical model
Ytest = array(test_outcomes)
print(f"Test outcomes shape is {Ytest.shape}")

# create model
print(f"{time.time() - startTime:.2f} : Creating model")
model = Sequential()
model.add(Dense(500, activation='relu', kernel_constraint=MaxNorm(3), input_shape=(max_review_words,)))
model.add(Dropout(0.1))
model.add(Dense(70, activation='relu', kernel_constraint=MaxNorm(3)))
model.add(Dropout(0.1))
model.add(Dense(30, activation='relu', kernel_constraint=MaxNorm(3)))
model.add(Dropout(0.1))
model.add(Dense(3, activation='softmax'))

# Compile model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(f"{time.time() - startTime:.2f} : Model compiled")

# fit model.  Include a 0.2 validation split
history = model.fit(Xtrain, Ytrain, validation_split=0.2, epochs=50, verbose=2)
print(f"{time.time() - startTime:.2f} : Model fitted")

# Evaluate the model on the test data
print("Checking against test set")
loss, acc = model.evaluate(Xtest, Ytest, verbose=0)
print('Test Accuracy: %f' % (acc))
print('Test Loss: %f' % (loss))

test_predictions = model.predict(Xtest)
test_predictions=argmax(test_predictions, axis=1)
cm = confusion_matrix (Ytest, test_predictions)
print('Confusion matrix.  True labels on rows, predicted on columns')
print(cm)

# Show metrics - model build progress
plt.plot (history.history['accuracy'])
plt.plot (history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

# Save the model
model.save(model_save_file)
print(f"{time.time() - startTime:.2f} : Model Saved")
