# Code to prep the data for the BOW model
#
# Created by Greg on 15/08/21
#
# Takes as input one or more files in the format generated by Data Preprocessing
# Creates a balanced data set in a single file including all fields in the original
# Adds a field including the year bin to ensure later stages
#
# At creation this is a copy of the one used for simple BOW, though in a new working directory

import json
from pathlib import Path
import time
import re

# Define the regex's to use for punctuation replacements
punc_set = {}
punc_stem = "xxpunc"
punc_set[punc_stem+'stop'] = re.compile(r"\.")
punc_set[punc_stem+'comma'] = re.compile(r",")
punc_set[punc_stem+'exclamation'] = re.compile(r"!")
punc_set[punc_stem+'colon'] = re.compile(r":")
punc_set[punc_stem+'semicolon'] = re.compile(r";")
punc_set[punc_stem+'dash'] = re.compile(r"-")
punc_set[punc_stem+'hash'] = re.compile(r"#")
punc_set[punc_stem+'question'] = re.compile(r"\?")
punc_set[punc_stem+'openbracket'] = re.compile(r"\(")
punc_set[punc_stem+'closebracket'] = re.compile(r"\)")
punc_set[punc_stem+'openbrace'] = re.compile(r"\{")
punc_set[punc_stem+'closebrace'] = re.compile(r"\}")
punc_set[punc_stem+'equal'] = re.compile(r"=")
punc_set[punc_stem+'greater'] = re.compile(r">")
punc_set[punc_stem+'less'] = re.compile(r"<")

# Function to do the prep.  Seperated as used for both train and test data
def prep_data (files_to_load, data_target_file, target_volume):

    print(f"\n\nCreating {data_target_file}\n")

    # Create the bins
    bins = {}
    bins[0] = {'2001', '2002', '2003', '2004', '2005'}
    bins[1] = {'2006', '2007', '2008', '2009', '2010'}
    bins[2] = {'2011', '2012', '2013', '2014', '2015'}
    bins[3] = {'2016', '2017', '2018', '2019', '2020'}

    # Determine the sample sizes
    target_volume_per_bin = int(target_volume / len(bins))
    print(f"For a target of {target_volume}, take {target_volume_per_bin} from each of the {len(bins)} bins")

    # Create the counters
    test_of_target_vol = 0 # Ensures loop only breaks when it should
    bin_count = {}
    year_counter = {}
    for i in range(len(bins)):
        bin_count[i] = int(target_volume_per_bin / len(bins[i]))
        for year in bins[i]:
            year_counter[year] = bin_count[i]
            test_of_target_vol += bin_count[i]
        print(f"Taking {bin_count[i]} from each of the {len(bins[i])} years in bin {i}")

    # Define a corpus sub-set
    corpus_sub_set = []

    # Read in the text corpus
    # This is now a list of JSONs, so needs to be read in accordingly
    target_count = 0

    for current_file in files_to_load:
        print(f"Starting processing of {current_file}...")
        file_to_read = pre_processed_data_source_folder / current_file
        with open(file_to_read, mode='r') as file:
            
            for line in file:

                review = json.loads(line.strip())
                for bin_id, bin in bins.items():
                    if ((review['review_year'] in bin) and (year_counter[review['review_year']] > 0)):
                        review['bin_id'] = bin_id

                        # Check for punctuation to replace
                        review['review_detail_original'] = review['review_detail']
                        for substitution, regex in punc_set.items():
                            review['review_detail'] = regex.sub(' '+substitution+' ', review['review_detail'])

                        corpus_sub_set.append(review)
                        year_counter[review['review_year']] = year_counter[review['review_year']] -1
                        if (year_counter[review['review_year']] == 0):
                            print(f"Found target number for {review['review_year']}")
                        target_count += 1

                if (target_count >= test_of_target_vol ):
                    break # Stop when enough data found

        if (target_count >= test_of_target_vol ):
            break # Stop when enough data found (outer loop)

    print(f"Selection complete after {time.time() - startTime:.2f} seconds. {target_count} reviews selected")

    # Write the result as a set of lines each of which is a JSON (allows streamed reading if required)
    with open(data_target_file, 'w', encoding='utf-8') as f:
        for review in corpus_sub_set:
            f.write(json.dumps(review))
            f.write('\n')

# Define general data source (true for train and test)
pre_processed_data_source_folder = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/pre_processed")

# General global for start time
startTime = time.time()

# Create training data
files_to_load = ["pre_processed_group_0.txt", "pre_processed_group_1.txt", "pre_processed_group_2.txt", "pre_processed_group_3.txt", "pre_processed_group_4.txt", "pre_processed_group_5.txt",  "pre_processed_group_6.txt", "pre_processed_group_7.txt"]
# Use the below instead of the above for testing			 
# files_to_load = ["pre_processed_group_0.txt"]
data_target_file = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/Word_embedding/training_data.txt")
training_volume = 300000
prep_data (files_to_load, data_target_file, target_volume = training_volume )

# Create test data
files_to_load = ["pre_processed_group_8.txt"]
data_target_file = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/Word_embedding/test_data.txt")
test_volume = 3000
prep_data (files_to_load, data_target_file, target_volume = test_volume )
