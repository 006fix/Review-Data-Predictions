# Code to generate a tokenized dictionary from the source.
#
# Created by Greg on 27/07/21
#
# Takes as input one or more files in the format generated by Data Preprocessing
# Tokenises using Keras 'Tokenizer' (see https://keras.io/api/preprocessing/text/#tokenizer)
# Spits out some statistics
# Stores the result as a JSON (see https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)
#
# NOTES
# Need to consider performance on words not in corpus vs training set.  Good use for the first test dat vs. second.
# So for now, build dictionary by tokenizing files 0-7, leaving out 8 and 9

import json
from pathlib import Path
import time
from keras.preprocessing.text import Tokenizer

# Define data source and target
data_source_folder = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/Word_embedding")
files_to_load = ["training_data.txt"]
# Below is useful if reading the original files
# files_to_load = ["BOW_training_data.json", "pre_processed_group_1.txt", "pre_processed_group_2.txt", "pre_processed_group_3.txt", "pre_processed_group_4.txt", "pre_processed_group_5.txt",  "pre_processed_group_6.txt", "pre_processed_group_7.txt"]
# Use the below instead of the above for testing			 
# files_to_load = ["pre_processed_group_0.txt"]

data_target_file = Path("C:/Users/gregp/Documents/kaggle/imdb-review-dataset/Word_embedding/tokenized_dictionary.json")

# create the tokenizer
# Most parameters are default, aside from the oov-token which is set to a specific entry so that rare words are visible
# List usage constrained to 10k words for now.  set below to 0 to use entire word list
# Seperates by capitalisation as that may align bette to word usage / position, and doesn't add much
# max_words = 10000
t = Tokenizer(filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower=False, split=' ', char_level=False, oov_token='oov_token')

# Read in the text corpus
# This is now a list of JSONs, so needs to be read in accordingly
startTime = time.time()
process_count = 0

for current_file in files_to_load:
    print(f"Starting load of {current_file}...")
    file_to_read = data_source_folder / current_file
    with open(file_to_read, mode='r') as file:
        
        process_count_file = 0

        # List to hold the working_set
        working_set = []
        
        for line in file:
            process_count_file +=1
            review = json.loads(line.strip())
            # Concatenate the three string fields into one field, then add that.  Done that way so that TFIDF aligns to the right number of documents
            # Uses 'movie_hidden_years' which hides the year of the film.  'movie' remains available for checking
            review_strings = review['movie_hidden_years'] + review['review_summary'] + review['review_detail']
            working_set.append(review_strings)
        
        print(f"Loaded {process_count_file} lines")
        process_count += process_count_file

        # fit the tokenizer on the reviews loaded
        print(f"Fitting tokenizer for {current_file}")
        t.fit_on_texts(working_set)
        print(f"Tokenizer fitted for {current_file} after {time.time() - startTime:.2f} seconds.")
        print(f"Processed {t.document_count} reviews, and found {len(t.word_index)} discrete words")

print(f"File loads complete after {time.time() - startTime:.2f} seconds. {process_count} reviews loaded")

# Dump the result as a JSON
tokenizer_json = t.to_json(indent=4)
with open(data_target_file, 'w', encoding='utf-8') as f:
    f.write(tokenizer_json + '\n')